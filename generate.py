#!/usr/bin/env python3

import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
from datetime import datetime, timedelta
import time

# --- CONFIGURATION ---
MODE = os.getenv("SCRAPER_MODE", "test5")  # "test5" | "all" | "new_only"
MAX_EPISODES = 5 if MODE == "test5" else None
pull_everything = MODE == "all"
pull_new_only = MODE == "new_only"
REQUEST_SLEEP = 1  # seconds between episode requests

BASE_HEADER = """<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd">
  <channel>
    <title>This American Archive</title>
    <link>https://www.thisamericanlife.org</link>
    <description>Autogenerated feed of the This American Life archive.</description>
    <language>en</language>
    <copyright>Copyright Â© Ira Glass / This American Life</copyright>
    <itunes:image href="https://i.imgur.com/0MMrLC4.png"/>
"""

BASE_FOOTER = """
  </channel>
</rss>
"""

FEED_FILE = "feed.xml"
existing_episodes = {}  # ep_num -> (title, original_air_date)

# --- LOAD EXISTING EPISODES ---
if os.path.exists(FEED_FILE):
    with open(FEED_FILE, "r", encoding="utf-8") as f:
        old_feed = BeautifulSoup(f.read(), "xml")
        for item in old_feed.find_all("item"):
            ep_tag = item.find("itunes:episode")
            title_tag = item.find("title")
            pub_date_tag = item.find("pubDate")
            if ep_tag and title_tag and pub_date_tag:
                ep_num = ep_tag.text.strip()
                title_text = title_tag.text.strip()
                pub_date_text = pub_date_tag.text.strip()
                existing_episodes[ep_num] = (title_text, pub_date_text)

# --- SETUP ---
archive_url = "https://www.thisamericanlife.org/archive"
session = requests.Session()
scrape_date = datetime.utcnow()
yesterday = scrape_date - timedelta(days=1)
items_html = ""

# --- SCRAPING LOOP ---
while archive_url:
    print(f"Fetching {archive_url}")
    r = session.get(archive_url)
    content = r.json()["html"] if "application/json" in r.headers.get("Content-Type", "") else r.content
    archive = BeautifulSoup(content, "html.parser")

    count = 0
    for episode_link in archive.select("header > a.goto-episode"):
        if MAX_EPISODES and count >= MAX_EPISODES:
            break

        full_url = urljoin("https://www.thisamericanlife.org", episode_link["href"])
        ep_slug = full_url.rstrip("/").split("/")[-1]
        print(f"Scraping episode {ep_slug}")

        r_episode = session.get(full_url)
        episode = BeautifulSoup(r_episode.content, "html.parser")

        # --- Episode date ---
        date_span = episode.select_one("span.date-display-single")
        episode_date = None
        original_air_date_str = ""
        if date_span:
            try:
                episode_date = datetime.strptime(date_span.text.strip(), "%B %d, %Y")
                original_air_date_str = episode_date.strftime("%Y-%m-%d")
            except Exception:
                pass

        # --- Playlist / audio ---
        title_meta = episode.select_one("script#playlist-data")
        if not title_meta:
            continue
        player_data = json.loads(title_meta.string)
        if "audio" not in player_data:
            continue
        ep_num = player_data["title"].split(":", 1)[0].strip()
        ep_title_base = ":".join(player_data["title"].split(":", 1)[1:]).strip()

        # --- Skip new_only if already in feed ---
        if pull_new_only and ep_num in existing_episodes:
            continue
        if pull_new_only and episode_date and episode_date.date() < yesterday.date():
            continue

        # --- Audio URLs ---
        audio_url = player_data["audio"]
        final_url = session.head(audio_url, allow_redirects=True).url
        parsed = urlparse(final_url)
        clean_url = urlunparse(parsed._replace(query=""))

        # Skip promos
        if "/promos/" in clean_url:
            continue

        # --- Determine repeat and clean labeling ---
        is_repeat = ep_num in existing_episodes
        clean_link_tag = episode.select_one('a[href*="clean"]')
        has_clean = bool(clean_link_tag)

        # Build episode title
        title_parts = []
        if has_clean:
            title_parts.append("Clean")
        if is_repeat:
            title_parts.append(f"Repeat {original_air_date_str}")
        if title_parts:
            ep_title = f"{ep_num}: {ep_title_base} ({'; '.join(title_parts)})"
        else:
            ep_title = f"{ep_num}: {ep_title_base}"

        # Explicit flag
        explicit_flag = "no" if has_clean else "yes"

        # --- Build description ---
        desc_parts = []
        meta_desc = episode.select_one("meta[name='description']")
        if meta_desc:
            desc_parts.append(meta_desc["content"].strip())

        for act in episode.select("div.field-items > div.field-item > article.node-act"):
            act_label_tag = act.select_one(".field-name-field-act-label .field-item")
            act_title_tag = act.select_one(".act-header a.goto-act")
            act_desc_tag = act.select_one(".field-name-body .field-item p")

            act_label = act_label_tag.text.strip() if act_label_tag else ""
            act_title = act_title_tag.text.strip() if act_title_tag else ""
            act_desc = act_desc_tag.text.strip() if act_desc_tag else ""

            if act_label.lower() == "prologue":
                act_text = act_label
                if act_title and act_title.lower() != "prologue":
                    act_text += f": {act_title}"
                if act_desc:
                    act_text += f"\n{act_desc}"
            else:
                act_text = f"{act_label}: {act_title}" if act_label and act_title else act_label or act_title
                if act_desc:
                    act_text += f"\n{act_desc}"

            if act_text:
                desc_parts.append(act_text)

        full_description = "\n\n".join(desc_parts)

        # --- Build item HTML ---
        pub_date_str = scrape_date.strftime("%a, %d %b %Y 00:00:00 +0000")
        item_block = f"""
    <item>
      <title>{ep_title}</title>
      <link>{full_url.strip()}</link>
      <itunes:episode>{ep_num}</itunes:episode>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>{explicit_flag}</itunes:explicit>
      <description>{full_description}</description>
      <pubDate>{pub_date_str}</pubDate>
      <enclosure url="{clean_url}" type="audio/mpeg"/>
    </item>
"""
        # Prepend newest first
        items_html = item_block + items_html
        existing_episodes[ep_num] = (ep_title, pub_date_str)
        count += 1

        # Optional clean version
        if has_clean:
            clean_audio_url = urljoin("https://www.thisamericanlife.org", clean_link_tag["href"])
            clean_item_block = f"""
    <item>
      <title>{ep_title}</title>
      <link>{full_url.strip()}</link>
      <itunes:episode>{ep_num}</itunes:episode>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:explicit>no</itunes:explicit>
      <description>{full_description}</description>
      <pubDate>{pub_date_str}</pubDate>
      <enclosure url="{clean_audio_url}" type="audio/mpeg"/>
    </item>
"""
            items_html = clean_item_block + items_html

        time.sleep(REQUEST_SLEEP)

    # --- Next page ---
    next_link = archive.select_one("a.pager")
    archive_url = urljoin("https://www.thisamericanlife.org", next_link["href"]) if pull_everything and next_link else None

# --- Write final feed ---
if MODE in ("test5", "all"):
    # overwrite feed
    with open(FEED_FILE, "w", encoding="utf-8") as f:
        f.write(BASE_HEADER)
        f.write(items_html)
        f.write(BASE_FOOTER)
elif MODE == "new_only":
    # append to existing feed
    if os.path.exists(FEED_FILE):
        with open(FEED_FILE, "r", encoding="utf-8") as f:
            existing_content = f.read()
        # inject new items before closing channel tag
        new_feed = existing_content.replace("</channel>", items_html + "\n  </channel>")
        with open(FEED_FILE, "w", encoding="utf-8") as f:
            f.write(new_feed)
    else:
        # fallback: write new feed if it doesn't exist
        with open(FEED_FILE, "w", encoding="utf-8") as f:
            f.write(BASE_HEADER)
            f.write(items_html)
            f.write(BASE_FOOTER)
